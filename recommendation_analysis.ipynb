{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install PyPDF2\n",
    "# %pip install pandas\n",
    "# %pip install re\n",
    "# %pip install openai==0.28\n",
    "# %pip install openpyxl\n",
    "# %pip install pdf2image pytesseract\n",
    "%pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import openai\n",
    "import os\n",
    "import glob\n",
    "from PyPDF2 import PdfReader\n",
    "import pandas as pd\n",
    "import re\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "import fitz  # PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the PDF file into text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_text_extracted_meaningful(text):\n",
    "    # Check if the text is too short or seems not meaningful\n",
    "    return len(text.strip()) > 100  # Adjust the threshold as needed\n",
    "\n",
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        # Try extracting text using PyMuPDF\n",
    "        text = \"\"\n",
    "        document = fitz.open(pdf_path)\n",
    "        for page_num in range(len(document)):\n",
    "            page = document.load_page(page_num)\n",
    "            extracted_text = page.get_text(\"text\")\n",
    "            if extracted_text:\n",
    "                text += extracted_text\n",
    "\n",
    "        # Fallback to OCR if text extraction is not meaningful\n",
    "        if not is_text_extracted_meaningful(text):\n",
    "            text = \"\"\n",
    "            images = convert_from_path(pdf_path)\n",
    "            for image in images:\n",
    "                text += pytesseract.image_to_string(image)\n",
    "        \n",
    "        text = text.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatGPT Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Personality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_extract_personality(statement):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o\",  # Use the appropriate model name\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Find out 10 keywords that describe this person personality based on their essay and generate the explanation of each personality based on the content put it with the header 'Personality and Explanation', if the content is in traditional chinese then the output should be in traditional chinese, if the content is in english then the output should be in english. please explain why do you output the keyword in that order with the header 'Order Explanation', and also explain straightforwardly your process of thinking, put it in the header 'Process of Thinking' \"},\n",
    "            {\"role\": \"user\", \"content\": statement},\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Extract the assistant's reply\n",
    "    assistant_reply = response['choices'][0]['message']['content']\n",
    "    return assistant_reply\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_analyzed_text(analyzed_text):\n",
    "    lines = analyzed_text.split('\\n')\n",
    "    parsed_data = []\n",
    "    current_keyword = \"\"\n",
    "    current_explanation = \"\"\n",
    "    \n",
    "    # Regular expression patterns to match different keyword formats\n",
    "    pattern1 = re.compile(r'\\d+\\.\\s*\\*\\*(.*?)\\*\\*')\n",
    "    pattern2 = re.compile(r'\\*\\*\\d+\\.\\s*(.*?):\\*\\*')\n",
    "    \n",
    "    for line in lines:\n",
    "        match1 = pattern1.match(line)\n",
    "        match2 = pattern2.match(line)\n",
    "        if match1:\n",
    "            if current_keyword and current_explanation:\n",
    "                parsed_data.append([current_keyword, current_explanation.strip()])\n",
    "            current_keyword = match1.group(1).strip()\n",
    "            current_explanation = line[match1.end():].strip()  # Start explanation from the rest of the line if present\n",
    "        elif match2:\n",
    "            if current_keyword and current_explanation:\n",
    "                parsed_data.append([current_keyword, current_explanation.strip()])\n",
    "            current_keyword = match2.group(1).strip()\n",
    "            current_explanation = line[match2.end():].strip()  # Start explanation from the rest of the line if present\n",
    "        else:\n",
    "            if current_keyword:\n",
    "                if line.strip():  # Only add lines that are not just empty or whitespace\n",
    "                    current_explanation += ' ' + line.strip()\n",
    "    \n",
    "    # Add the last keyword and explanation if present\n",
    "    if current_keyword and current_explanation:\n",
    "        parsed_data.append([current_keyword, current_explanation.strip()])\n",
    "    \n",
    "    return parsed_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Output to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_excel(parsed_data, ID, output_file):\n",
    "    # Create a DataFrame from the parsed data\n",
    "    df = pd.DataFrame(parsed_data, columns=['Personality', 'Explanation'])\n",
    "    \n",
    "    # Add a column for the file name\n",
    "    df.insert(0, 'File Name', ID)\n",
    "    \n",
    "    if os.path.exists(output_file):\n",
    "        # If the file exists, read the existing data\n",
    "        existing_df = pd.read_excel(output_file)\n",
    "        # Append the new data\n",
    "        df = pd.concat([existing_df, df], ignore_index=True)\n",
    "    \n",
    "    # Save the updated DataFrame back to the Excel file\n",
    "    df.to_excel(output_file, index=False)\n",
    "    print(f\"Results have been saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory traversal and processing\n",
    "base_dir = '2021-ET'\n",
    "sub_dirs = ['et-rejected']\n",
    "output_file = '2021et_rejected_GPTanalysis.xlsx'\n",
    "\n",
    "for sub_dir in sub_dirs:\n",
    "    sub_dir_path = os.path.join(base_dir, sub_dir)\n",
    "    print(f\"Processing subdirectory: {sub_dir_path}\")\n",
    "    \n",
    "    student_dirs = sorted(glob.glob(os.path.join(sub_dir_path, '2021-ET-*')))\n",
    "    \n",
    "    if not student_dirs:\n",
    "        print(f\"No student directories found in {sub_dir_path}\\n\")\n",
    "        continue\n",
    "        \n",
    "    for student_dir in student_dirs:\n",
    "        print(f\"Processing student directory: {student_dir}\\n\")\n",
    "        \n",
    "        pdf_files = sorted(glob.glob(os.path.join(student_dir, '*.pdf')))\n",
    "        if not pdf_files:\n",
    "            print(f\"No PDF files found in {student_dir}\")\n",
    "            continue\n",
    "        \n",
    "        concatenated_text = \"\"\n",
    "        for pdf_file in pdf_files:\n",
    "            print(f\"Reading PDF file: {pdf_file}\")\n",
    "            pdf_text = extract_text_from_pdf(pdf_file)\n",
    "            concatenated_text += pdf_text + \" \"\n",
    "        \n",
    "        print(f\"Concatenated text for {student_dir}:\\n{concatenated_text[:1000]}...\")  \n",
    "        print(f\"Total length of concatenated text: {len(concatenated_text)}\")\n",
    "        \n",
    "        analyzed_text = gpt_extract_personality(concatenated_text)\n",
    "        print(f\"Analyzed text for {student_dir}:\\n{analyzed_text}\\n\")\n",
    "        parsed_data = parse_analyzed_text(analyzed_text)\n",
    "        print(f\"Parsed data for {student_dir}:\\n{parsed_data}\\n\")\n",
    "        \n",
    "        student_dir_name = os.path.basename(student_dir)\n",
    "        save_to_excel(parsed_data, student_dir_name, output_file)\n",
    "\n",
    "print(f\"Results have been saved to {output_file}\")\n",
    "   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eecs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
