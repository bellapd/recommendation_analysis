{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install PyPDF2\n",
    "%pip install pandas\n",
    "%pip install re\n",
    "%pip install openai==0.28\n",
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import openai\n",
    "import os\n",
    "import glob\n",
    "from PyPDF2 import PdfReader\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the PDF file into text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "        text = text.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "    return text\n",
    "\n",
    "extract_text_from_pdf(\"sample_input.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatGPT Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Personality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_extract_personality(statement):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o\",  # Use the appropriate model name\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Find out 10 keywords that describe this person personality based on their essay and generate the explanation of each personality based on the content, if the content is in chinese then the output should be in chinese, if the content is in english then the output should be in english.\"},\n",
    "            {\"role\": \"user\", \"content\": statement},\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Extract the assistant's reply\n",
    "    assistant_reply = response['choices'][0]['message']['content']\n",
    "    return assistant_reply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_analyzed_text(analyzed_text):\n",
    "    lines = analyzed_text.split('\\n')\n",
    "    parsed_data = []\n",
    "    current_keyword = \"\"\n",
    "    current_explanation = \"\"\n",
    "    \n",
    "    pattern = re.compile(r'\\d+\\. \\*\\*(.*?)\\*\\*:')\n",
    "    \n",
    "    for line in lines:\n",
    "        match = pattern.match(line)\n",
    "        if match:\n",
    "            if current_keyword and current_explanation:\n",
    "                parsed_data.append([current_keyword, current_explanation.strip()])\n",
    "            current_keyword = match.group(1).strip()\n",
    "            current_explanation = line[match.end():].strip()\n",
    "        else:\n",
    "            if current_keyword:\n",
    "                current_explanation += ' ' + line.strip()\n",
    "    \n",
    "    if current_keyword and current_explanation:\n",
    "        parsed_data.append([current_keyword, current_explanation.strip()])\n",
    "    \n",
    "    return parsed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Output to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_excel(parsed_data, ID,output_file):\n",
    "    # Create a DataFrame from the parsed data\n",
    "    df = pd.DataFrame(parsed_data, columns=['Personality', 'Explanation'])\n",
    "    \n",
    "    # Add a column for the file name\n",
    "    df.insert(0, 'File Name', ID)\n",
    "    \n",
    "    with pd.ExcelWriter(output_file, mode='a', if_sheet_exists='overlay', engine='openpyxl') as writer:\n",
    "        # Check if the sheet exists and get the last row to append the new data\n",
    "        if 'Sheet1' in writer.sheets:\n",
    "            startrow = writer.sheets['Sheet1'].max_row\n",
    "        else:\n",
    "            startrow = 0\n",
    "        \n",
    "        df.to_excel(writer, index=False, header=startrow==0, startrow=startrow)\n",
    "\n",
    "    print(f\"Results have been saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory traversal and processing\n",
    "base_dir = '2019-C'\n",
    "sub_dirs = ['c-rejected']\n",
    "output_file = '2019c_rejected_GPTanalysis.xlsx'\n",
    "\n",
    "# Create an empty Excel file with the header if it doesn't exist\n",
    "if not os.path.exists(output_file):\n",
    "    df = pd.DataFrame(columns=['File Name', 'Personality', 'Explanation'])\n",
    "    df.to_excel(output_file, index=False)\n",
    "\n",
    "for sub_dir in sub_dirs:\n",
    "    sub_dir_path = os.path.join(base_dir, sub_dir)\n",
    "    print(f\"Processing subdirectory: {sub_dir_path}\")\n",
    "    student_dirs = glob.glob(os.path.join(sub_dir_path, '2019-A-*'))\n",
    "    if not student_dirs:\n",
    "        print(f\"No student directories found in {sub_dir_path}\")\n",
    "    for student_dir in student_dirs:\n",
    "        print(f\"Processing student directory: {student_dir}\")\n",
    "        \n",
    "        pdf_files = glob.glob(os.path.join(student_dir, '*.pdf'))\n",
    "        if not pdf_files:\n",
    "            print(f\"No PDF files found in {student_dir}\")\n",
    "        concatenated_text = \"\"\n",
    "        for pdf_file in pdf_files:\n",
    "            print(f\"Reading PDF file: {pdf_file}\")\n",
    "            pdf_text = extract_text_from_pdf(pdf_file)\n",
    "            print(f\"Extracted text length from {pdf_file}: {len(pdf_text)}\")\n",
    "            concatenated_text += pdf_text + \" \"\n",
    "        \n",
    "        # Print the concatenated text for verification\n",
    "        print(f\"Concatenated text for {student_dir}:\\n{concatenated_text[:1000]}...\")  # Print first 1000 characters for brevity\n",
    "        print(f\"Total length of concatenated text: {len(concatenated_text)}\")\n",
    "        \n",
    "        # Analyze concatenated text with ChatGPT\n",
    "        analyzed_text = gpt_extract_personality(concatenated_text)\n",
    "        \n",
    "        # Parse the analyzed text to get keywords and explanations\n",
    "        parsed_data = parse_analyzed_text(analyzed_text)\n",
    "        \n",
    "        # Get the student directory name\n",
    "        student_dir_name = os.path.basename(student_dir)\n",
    "        \n",
    "        # Save the parsed data to Excel\n",
    "        save_to_excel(parsed_data, student_dir_name, output_file)\n",
    "\n",
    "print(f\"Results have been saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eecs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
